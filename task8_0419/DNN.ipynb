{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本向量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "with open(\"cnews/cnews.train.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    train = file.readlines()\n",
    "with open(\"cnews/cnews.test.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    test = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactua(data):\n",
    "    random.shuffle(data) #数据集打乱\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for line in data:\n",
    "        x_data.append(line.replace('\\n', '').split('\\t')[1])\n",
    "        y_data.append(line.replace('\\n', '').split('\\t')[0])\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, y_train_data = exactua(train)\n",
    "x_test_data, y_test_data = exactua(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1000条 训练集\n",
    "# 100条 测试\n",
    "x_train_data, y_train_data = x_train_data[:1000], y_train_data[:10000]\n",
    "x_test_data, y_test_data= x_test_data[:100], y_test_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist():\n",
    "    import jieba \n",
    "    stopwords = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwordslist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(x_train_data, x_test_data):\n",
    "    #去停用词并进行结巴分词\n",
    "    import jieba\n",
    "    trainlists = []\n",
    "    for i in x_train_data:\n",
    "        word_list = [word for word in jieba.cut(i) if word not in stopwords]\n",
    "        trainlists.append(' '.join(word_list))\n",
    "    testlists = []\n",
    "    for i in x_test_data:\n",
    "        word_list = [word for word in jieba.cut(i) if word not in stopwords]\n",
    "        testlists.append(' '.join(word_list))\n",
    "    return trainlists, testlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = token(x_train_data, x_test_data)\n",
    "len(x_train)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2vec(x_train, x_test):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    count_vec=CountVectorizer(stop_words=stopwords) #创建词袋数据结构\n",
    "    X_count_train = count_vec.fit_transform(x_train)  #<class 'scipy.sparse.csr.csr_matrix'>\n",
    "    #train和test转化为特征向量\n",
    "    X_count_train= X_count_train.toarray()\n",
    "    X_count_test = count_vec.transform(x_test).toarray()   \n",
    "    return X_count_train, X_count_test, dict(count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, wordDic = data2vec(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 40938)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:\n",
      "\n",
      "\n",
      "研究 30194\n",
      "女人 15700\n",
      "眼泪 30010\n",
      "降低 39040\n",
      "男人 29226\n",
      "性欲 19636\n",
      "美人卷 32474\n",
      "珠帘 28772\n",
      "深坐 27134\n",
      "蛾眉 34340\n",
      "泪痕 26631\n"
     ]
    }
   ],
   "source": [
    "print ('vocabulary:\\n\\n')\n",
    "count = 0 \n",
    "for key,value in wordDic.items():\n",
    "    print(key,value)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2label(y_train, y_test):\n",
    "    dic = {}\n",
    "    count = 0\n",
    "    for i in set(y_train):\n",
    "        dic[i] = count \n",
    "        count += 1\n",
    "    y_train, y_test = [dic.get(i) for i in y_train_data], [dic.get(i) for i in y_test_data]\n",
    "    return y_train, y_test, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, dicLabel = y2label(y_train_data, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'时尚': 0,\n",
       " '财经': 1,\n",
       " '教育': 2,\n",
       " '娱乐': 3,\n",
       " '房产': 4,\n",
       " '家居': 5,\n",
       " '时政': 6,\n",
       " '科技': 7,\n",
       " '游戏': 8,\n",
       " '体育': 9}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知器网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    import numpy as np\n",
    "    import random \n",
    "    with open(\"cnews/cnews.train.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "        train = file.readlines()\n",
    "    with open(\"cnews/cnews.test.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "        test = file.readlines()\n",
    "    x_train_data, y_train_data = exactua(train)\n",
    "    x_test_data, y_test_data = exactua(test)    \n",
    "    # 1000条 训练集\n",
    "    # 100条 测试\n",
    "    x_train_data, y_train_data = x_train_data[:1000], y_train_data[:1000]\n",
    "    x_test_data, y_test_data= x_test_data[:100], y_test_data[:100]\n",
    "    stopwords = stopwordslist()\n",
    "    x_train, x_test = token(x_train_data, x_test_data)\n",
    "    x_train, x_test, wordDic = data2vec(x_train, x_test)\n",
    "    y_train, y_test, dicLabel = y2label(y_train_data, y_test_data)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 40760)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda\\envs\\py36tf14\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train = y_train[:,np.newaxis]\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "a=enc.fit_transform(y_train)\n",
    "y_train = a.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, x_train.shape[1]], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.int32, [None, 10], name='Y_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络参数\n",
    "n_hidden_1 = 128 # 第1个隐层  ，节点个数为256\n",
    "n_hidden_2 = 64 # 第2个隐层\n",
    "n_input =x_train.shape[1]# \n",
    "n_classes = 10 # \n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='W1'),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='W2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name='W')\n",
    "}\n",
    "\n",
    "#偏置在隐层的时候和节点个数一致，在输出层和输出类别个数一致\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]), name='bias')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此函数是用来构建计算的神经网络，得出预测类别的得分\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # 第1个隐层，使用relu激活函数\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'], name='fc_1')\n",
    "    layer_1 = tf.nn.relu(layer_1, name='relu_1')\n",
    "    # 第2个隐层，使用relu激活函数\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'], name='fc_2')\n",
    "    layer_2 = tf.nn.relu(layer_2, name='relu_2')\n",
    "    # 输出层\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'], name='fc_3')\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(X, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "loss_all = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y, name='cross_entropy_loss')\n",
    "loss = tf.reduce_mean(loss_all, name='avg_loss')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_aligned_list(data):\n",
    "    \"\"\"Shuffle arrays in a list by shuffling each array identically.\"\"\"\n",
    "    num = data[0].shape[0]\n",
    "    p = np.random.permutation(num)\n",
    "    return [d[p] for d in data]\n",
    "\n",
    "def batch_generator(data, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\n",
    "\n",
    "    Given a list of array-like objects, generate batches of a given\n",
    "    size by yielding a list of array-like objects corresponding to the\n",
    "    same slice of each input.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        data = shuffle_aligned_list(data)\n",
    "\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size > len(data[0]):\n",
    "            batch_count = 0\n",
    "\n",
    "            if shuffle:\n",
    "                data = shuffle_aligned_list(data)\n",
    "\n",
    "        start = batch_count * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_count += 1\n",
    "        yield [d[start:end] for d in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen = batch_generator([x_train, y_train], 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_x,batch_y = next(batch_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_test = y_test[:,np.newaxis]\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "a=enc.fit_transform(y_test)\n",
    "y_test = a.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1199.979976016\n",
      "Optimization Finished!\n",
      "Accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "#训练总轮数\n",
    "training_epochs = 1\n",
    "#一批数据大小\n",
    "batch_size = 1\n",
    "#信息展示的频度\n",
    "display_step = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # 训练\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(x_train.shape[0]/batch_size)\n",
    "        # 遍历所有的batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x,batch_y = next(batch_gen)\n",
    "            # 使用optimizer进行优化\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: batch_x, Y: batch_y})\n",
    "            # 求平均的损失\n",
    "            avg_loss += l / total_batch\n",
    "        # 每一步都展示信息\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_loss))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # 计算准确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: x_test, Y: y_test}))   #此处accuracy是Tensor，在运行计算时需要用到eval()\n",
    "    # print(\"Accuracy:\", sess.run(accuracy,feed_dict = {X: mnist.test.images, Y: mnist.test.labels}))    #也可以用sess.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内存限制，只做batch=1的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36tf11",
   "language": "python",
   "name": "py36tf11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
