{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本向量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "with open(\"cnews/cnews.train.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    train = file.readlines()\n",
    "with open(\"cnews/cnews.test.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    test = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactua(data):\n",
    "    random.shuffle(data) #数据集打乱\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for line in data:\n",
    "        x_data.append(line.replace('\\n', '').split('\\t')[1])\n",
    "        y_data.append(line.replace('\\n', '').split('\\t')[0])\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, y_train_data = exactua(train)\n",
    "x_test_data, y_test_data = exactua(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1000条 训练集\n",
    "# 100条 测试\n",
    "x_train_data, y_train_data = x_train_data[:1000], y_train_data[:1000]\n",
    "x_test_data, y_test_data= x_test_data[:100], y_test_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwordslist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(x_train_data, x_test_data):\n",
    "    #去停用词并进行结巴分词\n",
    "    trainlists = []\n",
    "    for i in x_train_data:\n",
    "        word_list = [word for word in jieba.cut(i) if word not in stopwords]\n",
    "        trainlists.append(' '.join(word_list))\n",
    "    testlists = []\n",
    "    for i in x_test_data:\n",
    "        word_list = [word for word in jieba.cut(i) if word not in stopwords]\n",
    "        testlists.append(' '.join(word_list))\n",
    "    return trainlists, testlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wttree\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.775 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test = token(x_train_data, x_test_data)\n",
    "len(x_train)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2vec(x_train, x_test):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    count_vec=CountVectorizer(stop_words=stopwords) #创建词袋数据结构\n",
    "    X_count_train = count_vec.fit_transform(x_train)  #<class 'scipy.sparse.csr.csr_matrix'>\n",
    "    #train和test转化为特征向量\n",
    "    X_count_train= X_count_train.toarray()\n",
    "    X_count_test = count_vec.transform(x_test).toarray()   \n",
    "    return X_count_train, X_count_test, dict(count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, wordDic = data2vec(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 41757)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:\n",
      "\n",
      "\n",
      "俄罗斯 8058\n",
      "加快 10889\n",
      "入世 8796\n",
      "步伐 26376\n",
      "优化 7428\n",
      "留学 30000\n",
      "环境 29309\n",
      "近日 37688\n",
      "世贸组织 5017\n",
      "谈判 36231\n",
      "美国 33162\n"
     ]
    }
   ],
   "source": [
    "print ('vocabulary:\\n\\n')\n",
    "count = 0 \n",
    "for key,value in wordDic.items():\n",
    "    print(key,value)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2label(y_train, y_test):\n",
    "    dic = {}\n",
    "    count = 0\n",
    "    for i in set(y_train):\n",
    "        dic[i] = count \n",
    "        count += 1\n",
    "    y_train, y_test = [dic.get(i) for i in y_train_data], [dic.get(i) for i in y_test_data]\n",
    "    return y_train, y_test, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test, dicLabel = y2label(y_train_data, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma=1.7, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', degree=2, gamma=1.7)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"训练集正确率\"+ str(sum(result==y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(x_test)\n",
    "sum(result==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-26-a9f8df63b6b9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-a9f8df63b6b9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print(\"训练集正确率\"+ str(sum(result==y_train)/len(y_train))\u001b[0m\n\u001b[1;37m                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print(\"测试集正确率\"+ str(sum(result==y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
