# 【**任务6 -传统机器学习--**SVM **】时长：2天**

## 1. SVM的原理

复习《统计学习方法》SVM推导

给定训练样本，支持向量机建立一个超平面作为决策曲面，使得正例和反例的隔离边界最大化。

凸优化问题最大化

SVM的核心思想就是找到不同类别之间的分界面，使得两类样本尽量落在面的两边，而且离分界面尽量远。

最早的SVM是平面的，局限很大。但是利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高SVM的适用范围。

## 2. SVM应用场景 

高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。

适用情景：

SVM在很多数据集上都有优秀的表现。

相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。

和随机森林一样，这也是一个拿到数据就可以先尝试一下的算法

## 3. SVM优缺点 

优点

可以解决高维问题，即大型特征空间；

能够处理非线性特征的相互作用；

无需依赖整个数据；

可以提高泛化能力；

需要对数据提前归一化，很多人使用的时候忽略了这一点，毕竟是基于距离的模型，所以LR也需要归一化

缺点

当观测样本很多时，效率并不是很高；

一个可行的解决办法是模仿随机森林，对数据分解，训练多个模型，然后求平均，时间复杂度降低p倍，分多少份，降多少倍

对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；

对缺失数据敏感；

对于核的选择也是有技巧的（libsvm中自带了四种核函数：线性核、多项式核、RBF以及sigmoid核）：

第一，如果样本数量小于特征数，那么就没必要选择非线性核，简单的使用线性核就可以了；

第二，如果样本数量大于特征数目，这时可以使用非线性核，将样本映射到更高维度，一般可以得到更好的结果；

第三，如果样本数目和特征数目相等，该情况可以使用非线性核，原理和第二种一样。

对于第一种情况，也可以先对数据进行降维，然后使用非线性核，这也是一种方法。

## 4. SVM sklearn 参数学习 

见当前文件夹下Bayes+SVM.ipynb

训练集正确率0.999，测试集正确率0.85

\5. 利用SVM模型结合 Tf-idf 算法进行文本分类