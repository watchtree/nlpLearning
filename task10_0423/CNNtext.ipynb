{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本向量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import datetime\n",
    "import collections\n",
    "with open(\"cnews/cnews.train.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    train = file.readlines()\n",
    "with open(\"cnews/cnews.test.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    test = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactua(data):\n",
    "    random.shuffle(data) #数据集打乱\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for line in data:\n",
    "        x_data.append(line.replace('\\n', '').split('\\t')[1])\n",
    "        y_data.append(line.replace('\\n', '').split('\\t')[0])\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, y_train_data = exactua(train)\n",
    "x_test_data, y_test_data = exactua(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1000条 训练集\n",
    "# 100条 测试\n",
    "x_train_data, y_train_data = x_train_data[:1000], y_train_data[:1000]\n",
    "x_test_data, y_test_data= x_test_data[:100], y_test_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist():\n",
    "    import jieba \n",
    "    stopwords = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    stopwords.append(' ')\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwordslist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitsentence(x_train_data, x_test_data):\n",
    "    #去停用词并进行结巴分词\n",
    "    import jieba\n",
    "    trainlists = []\n",
    "    for i in x_train_data:\n",
    "        word_list = [word for word in jieba.cut(i) if word not in stopwords]\n",
    "        trainlists.append(' '.join(word_list))\n",
    "    testlists = []\n",
    "    for i in x_test_data:\n",
    "        word_list = [word for word in jieba.cut(i) if word not in stopwords]\n",
    "        testlists.append(' '.join(word_list))\n",
    "    return trainlists, testlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\wttree\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.872 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = splitsentence(x_train_data, x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_count(d):\n",
    "    #字典排序\n",
    "    d = collections.OrderedDict(sorted(d.items(), key = lambda t: -t[1]))\n",
    "    return d\n",
    "\n",
    "def create_dict(data):\n",
    "    word_dic = {}\n",
    "    word_dic['PAD'] = 0\n",
    "    for i in data:\n",
    "        for word in i.split(' '):\n",
    "            if word == '':\n",
    "                continue\n",
    "            if word not in word_dic:\n",
    "                word_dic[word] = 1\n",
    "            else:\n",
    "                word_dic[word] += 1\n",
    "    word_dic = dict(sort_by_count(word_dic))\n",
    "    word2vecdic = {}\n",
    "    word2vecdic[\"PAD\"] = 0\n",
    "    count = 1\n",
    "    for i,j in word_dic.items():\n",
    "        if j > 5:\n",
    "            word2vecdic[i] = count\n",
    "            count += 1\n",
    "    return word2vecdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecdic = create_dict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(data, word2vecdic):\n",
    "    veclist = []\n",
    "    for i in data:\n",
    "        vec = [word2vecdic.get(i) if i in word2vecdic else 0 for i in i.split(' ')]\n",
    "        veclist.append(vec)\n",
    "    return veclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = token(x_train, word2vecdic)\n",
    "x_test = token(x_test, word2vecdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence size \n",
    "np.array([len(i) for i in x_train]).mean()\n",
    "sentence_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data, sentence_size):\n",
    "    for i in range(len(data)):\n",
    "        length = len(data[i])\n",
    "        if length > sentence_size:\n",
    "            data[i] = data[i][:sentence_size]\n",
    "        elif length < sentence_size:\n",
    "            data[i].extend([0]*(sentence_size-length))\n",
    "        else:\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = padding(x_train, sentence_size)\n",
    "x_test = padding(x_test, sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2label(y_train, y_test):\n",
    "    dic = {}\n",
    "    count = 0\n",
    "    for i in set(y_train):\n",
    "        dic[i] = count \n",
    "        count += 1\n",
    "    y_train, y_test = [dic.get(i) for i in y_train], [dic.get(i) for i in y_test]\n",
    "    return y_train, y_test, dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import collections\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    with open(\"cnews/cnews.train.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "        train = file.readlines()\n",
    "    with open(\"cnews/cnews.test.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "        test = file.readlines()   \n",
    "    x_train_data, y_train_data = exactua(train)\n",
    "    x_test_data, y_test_data = exactua(test)   \n",
    "    x_train_data, y_train_data = x_train_data[:1000], y_train_data[:1000]\n",
    "    x_test_data, y_test_data= x_test_data[:100], y_test_data[:100]\n",
    "    stopwords = stopwordslist()\n",
    "    x_train, x_test = splitsentence(x_train_data, x_test_data)\n",
    "    word2vecdic = create_dict(x_train)\n",
    "    x_train = token(x_train, word2vecdic)\n",
    "    x_test = token(x_test, word2vecdic)\n",
    "    sentence_size = 300\n",
    "    x_train = padding(x_train, sentence_size)\n",
    "    x_test = padding(x_test, sentence_size)\n",
    "    y_train, y_test, labeldic = y2label(y_train_data, y_test_data)\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = y_train[:,np.newaxis]\n",
    "    a=enc.fit_transform(y_train)\n",
    "    y_train = a.toarray()\n",
    "    y_test = np.array(y_test)\n",
    "    y_test= y_test[:,np.newaxis]\n",
    "    a=enc.fit_transform(y_test)\n",
    "    y_test = a.toarray()\n",
    "    return x_train, y_train, x_test, y_test, word2vecdic, labeldic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, word2vecdic, labeldic = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本卷积网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x_train.shape[1]\n",
    "classes_num = len(labeldic)\n",
    "vocabulary_size = len(word2vecdic)\n",
    "embedding_size = 300\n",
    "filters_height = [2, 3, 4]\n",
    "filter_num_per_height = [100, 100, 100]\n",
    "l2_lambda = 0.01\n",
    "batch_size = 3\n",
    "epochs = 2\n",
    "drop_keep_prob = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "    train_labels = tf.placeholder(tf.float32, [None, classes_num])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # embedding layer\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        conv_inputs = tf.expand_dims(embed, -1)\n",
    "        \n",
    "    features_pooled = []\n",
    "    for filter_height, filter_num in zip(filters_height, filter_num_per_height):\n",
    "        conv_filter = tf.Variable(tf.truncated_normal([filter_height, embedding_size, 1, filter_num], stddev=0.1))\n",
    "        \n",
    "        conv = tf.nn.conv2d(conv_inputs, conv_filter, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[filter_num]))\n",
    "        feature_map = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "        feature_pooled = tf.nn.max_pool(feature_map, ksize=[1, sequence_length - filter_height + 1, 1, 1],\n",
    "                                            strides=[1, 1, 1, 1],\n",
    "                                            padding='VALID')\n",
    "        features_pooled.append(feature_pooled)\n",
    "        \n",
    "    filter_num_total = sum(filter_num_per_height)\n",
    "    # fully connected layer\n",
    "    features_pooled_flat = tf.reshape(tf.concat(features_pooled, 3), [-1, filter_num_total])\n",
    "    features_pooled_flat_drop = tf.nn.dropout(features_pooled_flat, keep_prob) \n",
    "\n",
    "    W = tf.get_variable(\"W\", shape=[filter_num_total, classes_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[classes_num]))\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(features_pooled_flat_drop, W, b)\n",
    "    \n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=train_labels)\n",
    "    loss = tf.reduce_mean(losses) + l2_lambda * l2_loss\n",
    "\n",
    "    predictions = tf.argmax(scores, 1)\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(train_labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, num_epochs):\n",
    "    data = list(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(data_size / batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        data_shuffled = np.array(data)[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield data_shuffled[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25, Apr 2019 21:35:57: step 1, loss 24.8607, acc 0\n",
      "25, Apr 2019 21:35:57: step 2, loss 9.07715, acc 0.333333\n",
      "25, Apr 2019 21:35:57: step 3, loss 5.84646, acc 0.333333\n",
      "25, Apr 2019 21:35:57: step 4, loss 17.2226, acc 0\n",
      "25, Apr 2019 21:35:57: step 5, loss 18.5765, acc 0.333333\n",
      "25, Apr 2019 21:35:57: step 6, loss 17.0964, acc 0\n",
      "25, Apr 2019 21:35:58: step 7, loss 7.30108, acc 0.333333\n",
      "25, Apr 2019 21:35:58: step 8, loss 28.9857, acc 0\n",
      "25, Apr 2019 21:35:58: step 9, loss 27.0813, acc 0\n",
      "25, Apr 2019 21:35:58: step 10, loss 19.3843, acc 0\n",
      "25, Apr 2019 21:35:58: step 11, loss 17.5641, acc 0\n",
      "25, Apr 2019 21:35:58: step 12, loss 9.97443, acc 0.333333\n",
      "25, Apr 2019 21:35:59: step 13, loss 27.7803, acc 0\n",
      "25, Apr 2019 21:35:59: step 14, loss 20.5349, acc 0.333333\n",
      "25, Apr 2019 21:35:59: step 15, loss 9.93464, acc 0\n",
      "25, Apr 2019 21:35:59: step 16, loss 0.105621, acc 1\n",
      "25, Apr 2019 21:35:59: step 17, loss 16.0312, acc 0\n",
      "25, Apr 2019 21:35:59: step 18, loss 23.9938, acc 0\n",
      "25, Apr 2019 21:36:00: step 19, loss 15.8555, acc 0.333333\n",
      "25, Apr 2019 21:36:00: step 20, loss 4.05061, acc 0.666667\n",
      "25, Apr 2019 21:36:00: step 21, loss 2.76746, acc 0.666667\n",
      "25, Apr 2019 21:36:00: step 22, loss 13.1225, acc 0\n",
      "25, Apr 2019 21:36:00: step 23, loss 23.2268, acc 0.333333\n",
      "25, Apr 2019 21:36:00: step 24, loss 23.7466, acc 0\n",
      "25, Apr 2019 21:36:01: step 25, loss 10.0756, acc 0.333333\n",
      "25, Apr 2019 21:36:01: step 26, loss 14.8778, acc 0\n",
      "25, Apr 2019 21:36:01: step 27, loss 17.1558, acc 0.333333\n",
      "25, Apr 2019 21:36:01: step 28, loss 15.8709, acc 0\n",
      "25, Apr 2019 21:36:01: step 29, loss 9.37255, acc 0.666667\n",
      "25, Apr 2019 21:36:02: step 30, loss 7.68055, acc 0.333333\n",
      "25, Apr 2019 21:36:02: step 31, loss 21.3893, acc 0\n",
      "25, Apr 2019 21:36:02: step 32, loss 11.222, acc 0.333333\n",
      "25, Apr 2019 21:36:02: step 33, loss 13.9906, acc 0\n",
      "25, Apr 2019 21:36:02: step 34, loss 32.1253, acc 0\n",
      "25, Apr 2019 21:36:02: step 35, loss 27.9922, acc 0\n",
      "25, Apr 2019 21:36:03: step 36, loss 18.9696, acc 0\n",
      "25, Apr 2019 21:36:03: step 37, loss 22.0076, acc 0\n",
      "25, Apr 2019 21:36:03: step 38, loss 17.571, acc 0\n",
      "25, Apr 2019 21:36:03: step 39, loss 14.4154, acc 0\n",
      "25, Apr 2019 21:36:03: step 40, loss 7.81296, acc 0\n",
      "25, Apr 2019 21:36:04: step 41, loss 23.0653, acc 0\n",
      "25, Apr 2019 21:36:04: step 42, loss 9.26478, acc 0\n",
      "25, Apr 2019 21:36:04: step 43, loss 15.8119, acc 0\n",
      "25, Apr 2019 21:36:04: step 44, loss 15.1067, acc 0\n",
      "25, Apr 2019 21:36:04: step 45, loss 23.8526, acc 0\n",
      "25, Apr 2019 21:36:04: step 46, loss 17.9749, acc 0.333333\n",
      "25, Apr 2019 21:36:05: step 47, loss 12.8358, acc 0.333333\n",
      "25, Apr 2019 21:36:05: step 48, loss 15.3211, acc 0.333333\n",
      "25, Apr 2019 21:36:05: step 49, loss 16.2722, acc 0\n",
      "25, Apr 2019 21:36:05: step 50, loss 20.8634, acc 0\n",
      "25, Apr 2019 21:36:05: step 51, loss 17.7883, acc 0\n",
      "25, Apr 2019 21:36:05: step 52, loss 16.0776, acc 0\n",
      "25, Apr 2019 21:36:06: step 53, loss 5.80359, acc 0\n",
      "25, Apr 2019 21:36:06: step 54, loss 7.52497, acc 0.666667\n",
      "25, Apr 2019 21:36:06: step 55, loss 11.0159, acc 0\n",
      "25, Apr 2019 21:36:06: step 56, loss 3.78828, acc 0\n",
      "25, Apr 2019 21:36:06: step 57, loss 12.8116, acc 0\n",
      "25, Apr 2019 21:36:07: step 58, loss 20.6454, acc 0\n",
      "25, Apr 2019 21:36:07: step 59, loss 33.5565, acc 0\n",
      "25, Apr 2019 21:36:07: step 60, loss 38.3016, acc 0\n",
      "25, Apr 2019 21:36:07: step 61, loss 17.5481, acc 0.333333\n",
      "25, Apr 2019 21:36:07: step 62, loss 6.30833, acc 0.333333\n",
      "25, Apr 2019 21:36:08: step 63, loss 19.0976, acc 0\n",
      "25, Apr 2019 21:36:08: step 64, loss 26.5694, acc 0\n",
      "25, Apr 2019 21:36:08: step 65, loss 6.58088, acc 0\n",
      "25, Apr 2019 21:36:08: step 66, loss 28.9262, acc 0\n",
      "25, Apr 2019 21:36:08: step 67, loss 15.9456, acc 0\n",
      "25, Apr 2019 21:36:09: step 68, loss 27.1741, acc 0\n",
      "25, Apr 2019 21:36:09: step 69, loss 18.7048, acc 0\n",
      "25, Apr 2019 21:36:09: step 70, loss 20.7578, acc 0\n",
      "25, Apr 2019 21:36:09: step 71, loss 19.066, acc 0.333333\n",
      "25, Apr 2019 21:36:09: step 72, loss 16.4744, acc 0\n",
      "25, Apr 2019 21:36:09: step 73, loss 20.4947, acc 0\n",
      "25, Apr 2019 21:36:10: step 74, loss 11.0951, acc 0\n",
      "25, Apr 2019 21:36:10: step 75, loss 20.5367, acc 0\n",
      "25, Apr 2019 21:36:10: step 76, loss 18.442, acc 0\n",
      "25, Apr 2019 21:36:10: step 77, loss 21.6377, acc 0\n",
      "25, Apr 2019 21:36:10: step 78, loss 14.1265, acc 0.333333\n",
      "25, Apr 2019 21:36:10: step 79, loss 23.034, acc 0\n",
      "25, Apr 2019 21:36:11: step 80, loss 23.5896, acc 0\n",
      "25, Apr 2019 21:36:11: step 81, loss 7.47401, acc 0.333333\n",
      "25, Apr 2019 21:36:11: step 82, loss 23.7079, acc 0\n",
      "25, Apr 2019 21:36:11: step 83, loss 36.1401, acc 0\n",
      "25, Apr 2019 21:36:11: step 84, loss 20.3431, acc 0\n",
      "25, Apr 2019 21:36:12: step 85, loss 16.8652, acc 0.333333\n",
      "25, Apr 2019 21:36:12: step 86, loss 21.326, acc 0\n",
      "25, Apr 2019 21:36:12: step 87, loss 26.313, acc 0\n",
      "25, Apr 2019 21:36:12: step 88, loss 13.369, acc 0.333333\n",
      "25, Apr 2019 21:36:12: step 89, loss 16.3259, acc 0\n",
      "25, Apr 2019 21:36:12: step 90, loss 10.6996, acc 0\n",
      "25, Apr 2019 21:36:13: step 91, loss 7.70777, acc 0\n",
      "25, Apr 2019 21:36:13: step 92, loss 40.2263, acc 0\n",
      "25, Apr 2019 21:36:13: step 93, loss 4.44814, acc 0.333333\n",
      "25, Apr 2019 21:36:13: step 94, loss 30.342, acc 0\n",
      "25, Apr 2019 21:36:13: step 95, loss 16.3207, acc 0\n",
      "25, Apr 2019 21:36:13: step 96, loss 12.1572, acc 0.333333\n",
      "25, Apr 2019 21:36:14: step 97, loss 22.7576, acc 0\n",
      "25, Apr 2019 21:36:14: step 98, loss 16.6375, acc 0\n",
      "25, Apr 2019 21:36:14: step 99, loss 10.0262, acc 0\n",
      "25, Apr 2019 21:36:14: step 100, loss 24.7293, acc 0\n",
      "25, Apr 2019 21:36:14: step 101, loss 11.4097, acc 0\n",
      "25, Apr 2019 21:36:14: step 102, loss 16.6477, acc 0.333333\n",
      "25, Apr 2019 21:36:15: step 103, loss 12.506, acc 0\n",
      "25, Apr 2019 21:36:15: step 104, loss 25.2446, acc 0.333333\n",
      "25, Apr 2019 21:36:15: step 105, loss 16.7795, acc 0\n",
      "25, Apr 2019 21:36:15: step 106, loss 21.3698, acc 0\n",
      "25, Apr 2019 21:36:15: step 107, loss 24.3818, acc 0\n",
      "25, Apr 2019 21:36:16: step 108, loss 20.8351, acc 0\n",
      "25, Apr 2019 21:36:16: step 109, loss 12.775, acc 0.333333\n",
      "25, Apr 2019 21:36:16: step 110, loss 16.3838, acc 0\n",
      "25, Apr 2019 21:36:16: step 111, loss 19.3622, acc 0\n",
      "25, Apr 2019 21:36:16: step 112, loss 13.7374, acc 0\n",
      "25, Apr 2019 21:36:16: step 113, loss 20.6937, acc 0\n",
      "25, Apr 2019 21:36:17: step 114, loss 14.5967, acc 0\n",
      "25, Apr 2019 21:36:17: step 115, loss 5.92479, acc 0.333333\n",
      "25, Apr 2019 21:36:17: step 116, loss 21.3634, acc 0\n",
      "25, Apr 2019 21:36:17: step 117, loss 15.2303, acc 0\n",
      "25, Apr 2019 21:36:17: step 118, loss 7.26134, acc 0\n",
      "25, Apr 2019 21:36:17: step 119, loss 17.0895, acc 0\n",
      "25, Apr 2019 21:36:18: step 120, loss 13.9326, acc 0\n",
      "25, Apr 2019 21:36:18: step 121, loss 14.8149, acc 0.333333\n",
      "25, Apr 2019 21:36:18: step 122, loss 23.8589, acc 0\n",
      "25, Apr 2019 21:36:18: step 123, loss 18.0505, acc 0.333333\n",
      "25, Apr 2019 21:36:18: step 124, loss 6.97665, acc 0.333333\n",
      "25, Apr 2019 21:36:19: step 125, loss 9.20697, acc 0.333333\n",
      "25, Apr 2019 21:36:19: step 126, loss 20.576, acc 0.333333\n",
      "25, Apr 2019 21:36:19: step 127, loss 12.8214, acc 0.333333\n",
      "25, Apr 2019 21:36:19: step 128, loss 19.0693, acc 0\n",
      "25, Apr 2019 21:36:19: step 129, loss 26.7363, acc 0\n",
      "25, Apr 2019 21:36:19: step 130, loss 27.74, acc 0\n",
      "25, Apr 2019 21:36:20: step 131, loss 26.652, acc 0\n",
      "25, Apr 2019 21:36:20: step 132, loss 30.9331, acc 0\n",
      "25, Apr 2019 21:36:20: step 133, loss 23.8386, acc 0\n",
      "25, Apr 2019 21:36:20: step 134, loss 26.4003, acc 0\n",
      "25, Apr 2019 21:36:20: step 135, loss 19.3409, acc 0.333333\n",
      "25, Apr 2019 21:36:21: step 136, loss 9.10875, acc 0\n",
      "25, Apr 2019 21:36:21: step 137, loss 8.90172, acc 0.333333\n",
      "25, Apr 2019 21:36:21: step 138, loss 15.7696, acc 0\n",
      "25, Apr 2019 21:36:21: step 139, loss 21.6185, acc 0\n",
      "25, Apr 2019 21:36:21: step 140, loss 14.5582, acc 0\n",
      "25, Apr 2019 21:36:22: step 141, loss 29.0023, acc 0\n",
      "25, Apr 2019 21:36:22: step 142, loss 24.9053, acc 0\n",
      "25, Apr 2019 21:36:22: step 143, loss 20.0058, acc 0\n",
      "25, Apr 2019 21:36:22: step 144, loss 15.0198, acc 0\n",
      "25, Apr 2019 21:36:22: step 145, loss 16.6067, acc 0\n",
      "25, Apr 2019 21:36:23: step 146, loss 15.1392, acc 0\n",
      "25, Apr 2019 21:36:23: step 147, loss 27.7875, acc 0\n",
      "25, Apr 2019 21:36:23: step 148, loss 11.1679, acc 0\n",
      "25, Apr 2019 21:36:23: step 149, loss 16.6782, acc 0\n",
      "25, Apr 2019 21:36:23: step 150, loss 19.2086, acc 0\n",
      "25, Apr 2019 21:36:23: step 151, loss 20.002, acc 0\n",
      "25, Apr 2019 21:36:24: step 152, loss 15.1632, acc 0.333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25, Apr 2019 21:36:24: step 153, loss 16.6318, acc 0.333333\n",
      "25, Apr 2019 21:36:24: step 154, loss 14.3632, acc 0\n",
      "25, Apr 2019 21:36:24: step 155, loss 14.7603, acc 0\n",
      "25, Apr 2019 21:36:24: step 156, loss 8.99372, acc 0.666667\n",
      "25, Apr 2019 21:36:25: step 157, loss 26.745, acc 0\n",
      "25, Apr 2019 21:36:25: step 158, loss 4.44551, acc 0.333333\n",
      "25, Apr 2019 21:36:25: step 159, loss 16.4985, acc 0\n",
      "25, Apr 2019 21:36:25: step 160, loss 18.9258, acc 0\n",
      "25, Apr 2019 21:36:25: step 161, loss 23.3052, acc 0\n",
      "25, Apr 2019 21:36:26: step 162, loss 21.0449, acc 0.333333\n",
      "25, Apr 2019 21:36:26: step 163, loss 28.6778, acc 0\n",
      "25, Apr 2019 21:36:26: step 164, loss 18.5459, acc 0\n",
      "25, Apr 2019 21:36:26: step 165, loss 14.1849, acc 0\n",
      "25, Apr 2019 21:36:26: step 166, loss 11.9146, acc 0\n",
      "25, Apr 2019 21:36:27: step 167, loss 42.212, acc 0\n",
      "25, Apr 2019 21:36:27: step 168, loss 33.8663, acc 0\n",
      "25, Apr 2019 21:36:27: step 169, loss 15.5751, acc 0\n",
      "25, Apr 2019 21:36:27: step 170, loss 17.2227, acc 0.333333\n",
      "25, Apr 2019 21:36:27: step 171, loss 20.8324, acc 0\n",
      "25, Apr 2019 21:36:28: step 172, loss 12.1676, acc 0\n",
      "25, Apr 2019 21:36:28: step 173, loss 11.3854, acc 0.333333\n",
      "25, Apr 2019 21:36:28: step 174, loss 11.5285, acc 0.333333\n",
      "25, Apr 2019 21:36:28: step 175, loss 13.4618, acc 0.333333\n",
      "25, Apr 2019 21:36:28: step 176, loss 12.7523, acc 0.333333\n",
      "25, Apr 2019 21:36:29: step 177, loss 11.1739, acc 0\n",
      "25, Apr 2019 21:36:29: step 178, loss 8.75017, acc 0\n",
      "25, Apr 2019 21:36:29: step 179, loss 21.4961, acc 0\n",
      "25, Apr 2019 21:36:29: step 180, loss 22.0913, acc 0\n",
      "25, Apr 2019 21:36:29: step 181, loss 20.2272, acc 0\n",
      "25, Apr 2019 21:36:30: step 182, loss 27.615, acc 0\n",
      "25, Apr 2019 21:36:30: step 183, loss 8.94405, acc 0\n",
      "25, Apr 2019 21:36:30: step 184, loss 11.7736, acc 0\n",
      "25, Apr 2019 21:36:30: step 185, loss 18.8259, acc 0\n",
      "25, Apr 2019 21:36:30: step 186, loss 22.3998, acc 0\n",
      "25, Apr 2019 21:36:30: step 187, loss 28.5676, acc 0\n",
      "25, Apr 2019 21:36:31: step 188, loss 8.85255, acc 0\n",
      "25, Apr 2019 21:36:31: step 189, loss 12.1557, acc 0\n",
      "25, Apr 2019 21:36:31: step 190, loss 29.7955, acc 0\n",
      "25, Apr 2019 21:36:31: step 191, loss 5.169, acc 0.333333\n",
      "25, Apr 2019 21:36:31: step 192, loss 22.0067, acc 0.333333\n",
      "25, Apr 2019 21:36:32: step 193, loss 18.6127, acc 0\n",
      "25, Apr 2019 21:36:32: step 194, loss 16.8928, acc 0\n",
      "25, Apr 2019 21:36:32: step 195, loss 14.7785, acc 0\n",
      "25, Apr 2019 21:36:32: step 196, loss 21.6395, acc 0\n",
      "25, Apr 2019 21:36:32: step 197, loss 2.5106, acc 0.333333\n",
      "25, Apr 2019 21:36:33: step 198, loss 11.642, acc 0\n",
      "25, Apr 2019 21:36:33: step 199, loss 26.4555, acc 0\n",
      "25, Apr 2019 21:36:33: step 200, loss 11.3428, acc 0.333333\n",
      "25, Apr 2019 21:36:33: step 201, loss 19.3702, acc 0\n",
      "25, Apr 2019 21:36:33: step 202, loss 22.8161, acc 0\n",
      "25, Apr 2019 21:36:34: step 203, loss 17.111, acc 0\n",
      "25, Apr 2019 21:36:34: step 204, loss 12.7554, acc 0.333333\n",
      "25, Apr 2019 21:36:34: step 205, loss 21.3331, acc 0\n",
      "25, Apr 2019 21:36:34: step 206, loss 7.78209, acc 0.333333\n",
      "25, Apr 2019 21:36:34: step 207, loss 7.71987, acc 0.333333\n",
      "25, Apr 2019 21:36:34: step 208, loss 12.9309, acc 0\n",
      "25, Apr 2019 21:36:35: step 209, loss 26.8168, acc 0\n",
      "25, Apr 2019 21:36:35: step 210, loss 11.2308, acc 0\n",
      "25, Apr 2019 21:36:35: step 211, loss 13.8149, acc 0\n",
      "25, Apr 2019 21:36:35: step 212, loss 11.992, acc 0\n",
      "25, Apr 2019 21:36:35: step 213, loss 12.8446, acc 0.333333\n",
      "25, Apr 2019 21:36:36: step 214, loss 6.23153, acc 0\n",
      "25, Apr 2019 21:36:36: step 215, loss 18.213, acc 0.333333\n",
      "25, Apr 2019 21:36:36: step 216, loss 24.8856, acc 0\n",
      "25, Apr 2019 21:36:36: step 217, loss 32.4541, acc 0\n",
      "25, Apr 2019 21:36:36: step 218, loss 14.4973, acc 0.333333\n",
      "25, Apr 2019 21:36:36: step 219, loss 6.75344, acc 0.333333\n",
      "25, Apr 2019 21:36:37: step 220, loss 11.3287, acc 0\n",
      "25, Apr 2019 21:36:37: step 221, loss 10.7103, acc 0\n",
      "25, Apr 2019 21:36:37: step 222, loss 22.9272, acc 0\n",
      "25, Apr 2019 21:36:37: step 223, loss 30.5744, acc 0.333333\n",
      "25, Apr 2019 21:36:37: step 224, loss 15.5012, acc 0\n",
      "25, Apr 2019 21:36:38: step 225, loss 14.5301, acc 0.333333\n",
      "25, Apr 2019 21:36:38: step 226, loss 15.9637, acc 0\n",
      "25, Apr 2019 21:36:38: step 227, loss 14.9679, acc 0.333333\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss, aggregation_method=2)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    batches = get_batch(zip(x_train, y_train), batch_size, epochs)\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        feed_dict = {train_inputs: x_batch, train_labels: y_batch, keep_prob: drop_keep_prob}\n",
    "        _, step, _loss, _accuracy = sess.run([train_op, global_step, loss, accuracy], feed_dict)\n",
    "        time_str = datetime.datetime.now().strftime(\"%d, %b %Y %H:%M:%S\")\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, _loss, _accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36tf14]",
   "language": "python",
   "name": "conda-env-py36tf14-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
